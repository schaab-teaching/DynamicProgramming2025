\documentclass[11pt, aspectratio=169]{beamer}

\usepackage{amsmath, amsfonts, microtype, nicefrac, amssymb, amsthm, centernot}

\usepackage{pgfpages}

\usepackage{helvet}
\usepackage[default]{lato}
\usepackage{array}

\usefonttheme[onlymath]{serif}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{bm}

\usepackage{mathpazo}
\usepackage{hyperref}
\usepackage{multimedia}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bbm}
\newcolumntype{d}[0]{D{.}{.}{5}}

\usepackage{graphicx}
\usepackage[space]{grffile}
\usepackage{booktabs}

\usepackage{setspace}

\usepackage{transparent}


%%% FIGURES %%%
\usepackage{caption, subcaption}
\usepackage{booktabs, siunitx}
\usepackage{pgfplots} 
%\usepackage[outdir=./figures]{epstopdf}
\usepackage{float}
\usepackage{graphicx}
\usepackage[absolute, overlay]{textpos}
\usepackage{epstopdf}


%%% TIKZ %%%
\usepackage{tikz}
\usepackage{verbatim}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}
\usetikzlibrary{bending}
\usetikzlibrary{snakes}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{shapes.misc}
\usetikzlibrary{matrix, shapes, arrows, fit, tikzmark}


%%% ALGORITHM %%%
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{multimedia}


%%% APPENDIX SLIDE NUMBERING %%%
\usepackage{appendixnumberbeamer}


%%% BEAMER BUTTON %%%
%\setbeamertemplate{button}{\tikz
	%\node[
	%	inner xsep = 2pt, 
	%	draw = structure!0, 
	%	fill = myblue, 
	%	rounded corners = 4pt]{\color{white} \tiny\insertbuttontext};
	%}


%%% COLORS %%%
\definecolor{blue}{RGB}{0,38,118}
\definecolor{red}{RGB}{213,94,0}
\definecolor{yellow}{RGB}{240,228,66}
\definecolor{green}{RGB}{0,158,115}

\definecolor{myred}{RGB}{163,32,45}
\definecolor{navyblue}{rgb}{0.05,0.2,0.70}
\definecolor{myblue}{RGB}{0,51,150}
\definecolor{myorange}{RGB}{255,140,0}
\definecolor{myref}{RGB}{160,160,160}
\definecolor{shock}{RGB}{0, 125, 34}%{50, 168, 82}

\definecolor{background}{RGB}{255,253,218}

% Define a new transparent color
\definecolor{trans}{rgb}{1,1,1}
\colorlet{trans}{black!20} % 0 percent opacity

\hypersetup{
  colorlinks=false,
  linkbordercolor = {white},
  linkcolor = {blue}
}

\setbeamercolor{frametitle}{fg=blue}
\setbeamercolor{title}{fg=black}
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{itemize items}{-}
\setbeamercolor{itemize item}{fg=blue}
\setbeamercolor{itemize subitem}{fg=blue}
\setbeamercolor{enumerate item}{fg=blue}
\setbeamercolor{enumerate subitem}{fg=blue}
\setbeamercolor{button}{bg=background, fg=blue,}

%\setbeamercolor{background canvas}{bg=background}


%%% FRAME TITLE %%%
\setbeamerfont{title}{series=\bfseries, parent=structure}
\setbeamerfont{frametitle}{series=\bfseries, parent=structure}


%%% TRANSITION FRAME %%%
\newenvironment{transitionframe}{
	\setbeamercolor{background canvas}{bg=blue}
	\begin{frame}
		\thispagestyle{empty}
		\addtocounter{framenumber}{-1}
		\vspace{42mm}
		\hspace{4mm} }{
		\begin{tikzpicture}
			\tikz \fill [white] (1,6) rectangle (20,10);
		\end{tikzpicture}
	\end{frame}
}


%%% OUTLINE %%%
\AtBeginSection[]
{
	\begin{frame}
       \frametitle{Roadmap of Talk}
       \tableofcontents[currentsection]
   \end{frame}
}
\setbeamercolor{section in toc}{fg=blue}
\setbeamercolor{subsection in toc}{fg=red}
\setbeamersize{text margin left=1em,text margin right=1em} 


%%% ENVIRONMENTS
\newenvironment{witemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}

\makeatother
\setbeamertemplate{itemize items}{\large\raisebox{0mm}{\textbullet}}
\setbeamertemplate{itemize subitem}{\footnotesize\raisebox{0.15ex}{--}}
\setbeamertemplate{itemize subsubitem}{\Tiny\raisebox{0.7ex}{$\blacktriangleright$}}

\setbeamertemplate{enumerate item}[default]
\setbeamertemplate{enumerate subitem}{\textbullet}
\makeatletter

% ITEMIZE SPACING:
% \usepackage{xpatch}
% \xpatchcmd{\itemize}
% {\def\makelabel}
% {\setlength{\itemsep}{0mm}\def\makelabel}
% {}
% {}


%%% PRETTY ENUMERATE %%%
% \usepackage{stackengine,xcolor}
% \newcommand\circnum[2]{\stackinset{c}{}{c}{.1ex}{\small\textcolor{white}{#2}}%
	% 	{\abovebaseline[-.7ex]{\Huge\textcolor{#1}{$\bullet$}}}}
% \newenvironment{myenum}
% {\let\svitem\item
	% 	\renewcommand\item[1][black]{%
		% 		\refstepcounter{enumi}\svitem[\circnum{##1}{\theenumi}]}%
	% 	\begin{enumerate}}{\end{enumerate}}
\usepackage{stackengine,xcolor,graphicx}
\newcommand\circnum[2]{\smash{\stackinset{c}{}{c}{.2ex}{\small\textcolor{white}{#2}}%
		{\abovebaseline[-1.1ex]{\Huge\textcolor{#1}{\scalebox{1.5}{$\bullet$}}}}}}
\newenvironment{myenum}
{\let\svitem\item
	\renewcommand\item[1][black]{%
		\refstepcounter{enumi}\svitem[\circnum{##1}{\theenumi}]}%
	\begin{enumerate}}{\end{enumerate}}

\newcommand{\notimplies}{\;\not\!\!\!\implies}



%%%%%%%%%%%%%%%%%%%%%%%%%%  TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[]{\\[8pt]
	{\large \color{blue} Dynamic Programming and Applications \\[5pt] \normalfont{Discrete Time Dynamics and Optimization} \\[10pt] \normalfont{Lecture 2}}}
\author[Schaab]{Andreas Schaab}
\institute{}
\subject{}
\date{}



%%%%%%%%%%%%%%%%%%%%%%%%  BEGIN DOC   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%% TIKZ %%% 
\tikzstyle{every picture}+=[remember picture]
%\everymath{\displaystyle}

\tikzset{   
	every picture/.style={remember picture,baseline},
	every node/.style={anchor=base,align=center,outer sep=1.5pt},
	every path/.style={thick},
}
\newcommand\marktopleft[1]{%
	\tikz[overlay,remember picture] 
	\node (marker-#1-a) at (-.3em,.3em) {};%
}
\newcommand\markbottomright[2]{%
	\tikz[overlay,remember picture] 
	\node (marker-#1-b) at (0em,0em) {};%
}
\tikzstyle{every picture}+=[remember picture] 
\tikzstyle{mybox} =[draw=black, very thick, rectangle, inner sep=10pt, inner ysep=20pt]
\tikzstyle{fancytitle} =[draw=black,fill=red, text=white]


\addtocounter{framenumber}{-1}
\thispagestyle{empty}
\maketitle 
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}

Last lecture, we introduced the Bellman equation:
\begin{equation*}
	V(k) = \max_{k'} \Big\{ u\Big( f(k) - k' \Big) + \beta V(k') \Big\}
\end{equation*}

\vspace{5mm}
\begin{witemize}
\item The value today is = flow payoff + continuation value

\item This assumes there is no uncertainty: We can make perfect forecasts about the future

	{\footnotesize Not how the world works!} 

\item Suppose production also depends on productivity $y_t = f(k_t, A_t)$ and $A_{t+1}$ is uncertain
\end{witemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}

In an uncertain world, we care about the \textit{expected} continuation value
\begin{equation*}
	V(k, {\color{blue} A}) = \max_{k'} \Big\{ u\Big( f(k, {\color{blue} A}) - k' \Big) + \beta {\color{blue} \mathbb E} V(k', {\color{blue} A'}) \Big\}
\end{equation*}

\vspace{5mm}
\begin{witemize}
\item Now the only question is: How do we compute the expectation $\mathbb E$?

\item We have to study stochastic processes (and stochastic calculus) to answer this  
\end{witemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Outline}
\thispagestyle{empty}
\addtocounter{framenumber}{-1}

Part 1: Difference equations
\begin{enumerate}
	\item Stochastic processes
	\item Markov chains
	\item Difference equations
	\item Stochastic difference equations
\end{enumerate}

\vspace{5mm} 
Part 2: Stochastic dynamic programming
\begin{enumerate}
	\item Stochastic dynamic programming
	\item History notation
	\item The stochastic neoclassical growth model
\end{enumerate}

\vspace{5mm} 
Part 3: Optimal stopping 
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{transitionframe}
	{\color{white} \Huge \textbf{Part 1: Difference Equations} \vspace{2mm}}
\end{transitionframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{1. Stochastic processes}
\begin{witemize}
\item Let $X_t$ be a random variable that is time $t$ adapted

\item Discrete time: We index time discretely $t = 0, 1, 2, \ldots, T \leq \infty$

\item Stochastic process in discrete time: a sequence of random variables indexed by $t$, $\{X_t\}_{t=0}^T$

\item Continuous time: We index time continuously $t \in [0, T]$ with $T \leq \infty$

\item Stochastic process in continuous time: a sequence of random variables indexed by $t$, $\{X_t\}_{t \geq 0}$

\end{witemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{2. Markov chains}
\begin{witemize}
\item A stochastic process $\{X_t\}$ has the \textit{Markov property} if for all $k \geq 1$ and all $t$:
\begin{equation*}
	\mathbb P(X_{t+1} = x \mid X_t, X_{t-1}, \ldots, X_{t-k}) = \mathbb P(X_{t+1} = x \mid X_t)
\end{equation*}

\item \textit{State space} of the Markov process = set of events or states that it visits 

\item A Markov chain is a Markov process (stochastic process with Markov property) that visits a finite number of states (\textit{discrete state space})

\item Simplest example: Individual $i$ is randomly hit by earnings (employment) shocks and switches between $X_t \in \{X^L, X^H\}$
\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\begin{witemize}
\item Markov chains have a \textit{transition matrix} $P$ that describes the probability of transitioning from state $i$ to state $j$

\item Simplest example with state space $\{X^L, X^H\}$
\begin{equation*}
	P = \begin{pmatrix} 0.8 & 0.2 \\ 0.2 & 0.8 \end{pmatrix}
\end{equation*}

\item This says: P of staying in employment state = $0.8$, P of switching = $0.2$

\item $P_{ij}$ is the probability of switching from state $i$ to state $j$ (one period)

\item $P^2$ characterizes transitions over two periods: $(P^2)_{ij}$ is prob of going from $i$ to $j$ in two periods

\item The rows of the transition matrix have to sum to $1$ (definition of probability measure)
\end{witemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{3. Difference equations}
\begin{witemize}
\item We start with deterministic (non-random) dynamics and then conclude with stochastic (random) dynamics

\item The \textit{first-order linear difference equation} is defined by
\begin{equation}\label{eq:linear_difference}
	x_{t+1} = b x_t + c z_t 
\end{equation}
where $\{z_t\}$ is an exogenously given, bounded sequence

\item For now, all objects are (real) scalars (easy to extend to vectors and matrices)

\item Suppose we have an \textit{initial condition} (i.e., given initial value) $x_0$

\item When $c = 0$, \eqref{eq:linear_difference} is a \textit{time-homogeneous} difference equation

\item When $cz_t$ is constant for all $t$, \eqref{eq:linear_difference} is an \textit{autonomous} difference equation

\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Autonomous equations}
\begin{witemize}
\item Consider the autonomous equation with $z_t = 1$ 

\item A particular solution is the constant solution with $x_t = \frac{c}{1-b}$ when $b \neq 1$

\item Such a point is called a \textit{stationary point} or \textit{steady state}

\item General solution of the autonomous equation (for some constant $x$):
\begin{equation}\label{eq:linear_difference_solution}
	x_t = (x_0 - x) b^t + x
\end{equation}

\item Important question is long-run behavior (stability / convergence)

\item When $| b | < 1$, \eqref{eq:linear_difference_solution} converges asymptotically to steady state $x$ for any initial value $x_0$ (steady state $x$ is globally stable) 

\item If $| b | > 1$, \eqref{eq:linear_difference_solution} explodes and is not stable (except when $x_0 = x$)
\end{witemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Examples in macro}

\textbf{Capital accumulation}:
\begin{equation*}
	K_{t+1} = (1 - \delta) K_t + I_t
\end{equation*}
\begin{witemize}
\item $\delta$ is depreciation and $I_t$ is investment

\item This is a \textit{forward equation} and requires an initial condition $K_0$

\item If $I_t = 0$ and $0 < \delta < 1$, $K_t \to 0$

\item If $I_t = c$ constant, then $K_t$ converges to $\frac{c}{\delta}$: $K_{t+1} = (1-\delta) \frac{c}{\delta} + c = \frac{c}{\delta}$
\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\textbf{Wealth dynamics}:
\begin{equation*}
	a_{t+1} = R_t a_t + y_t - c_t
\end{equation*}
\begin{witemize}
	\item $R_t$ is the gross real interest rate, $y_t$ is income, $c_t$ is consumption
	
	\item This is a \textit{forward equation} and requires an initial condition $a_0$
	
	\item We will study this as a \textit{controlled} process because $c_t$ will be chosen optimally
	
	\item Work out the following: $R_t = R$ and $y_t = y$ constant, and  
	\begin{equation*}
		c_t = \bigg( 1 - \frac{1}{R} \bigg) \bigg(a_t + \sum_{s=t}^\infty R^{-(s-t)} y \bigg)
	\end{equation*}
	What are the dynamics of $a_t$?
\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\textbf{Consumption Euler equation}:
\begin{equation*}
	\frac{1}{C_t} = \beta R_t\frac{1}{C_{t+1}}
\end{equation*}
\begin{witemize}
	\item $\frac{1}{C_t} = u'(C_t)$ is marginal utility with log preferences
	
	\item This is a \textit{backward equation} and requires a terminal condition or transversality condition, i.e., $c_T$ must converge to something
	
	\item Suppose there exists time $T$ s.t. for all $t \geq T$, $C_t = C$
	
	\item Then solve \textit{backwards} from: $\frac{1}{C_{T-1}} = \beta R_{T-1} \frac{1}{C_T}$ or expressed as \textit{time-homogeneous first-order linear difference equation}
	\begin{equation*}
		C_{T-1} = \frac{1}{\beta R_{T-1}} C_T
	\end{equation*}

	\item Difference between \textit{forward} and \textit{backward} equations is critical! This is closely related to the idea of \textit{boundary conditions} (much more to come)
\end{witemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{4. Stochastic difference equations}
\begin{witemize}
\item Consider the process $\{X_t\}$ with 
\begin{equation}\label{eq:stochastic_difference}
	X_{t+1} = A X_t + Cw_{t+1}
\end{equation}
where $w_{t+1}$ is an iid. process with $w_{t+1} \sim \mathcal N(0, 1)$

\item Equation \eqref{eq:stochastic_difference} is a \textit{first-order, linear stochastic difference equation}

\item Let $\mathbb E_t$ the \textit{conditional expectation} operator (conditional on time $t$ information)

\item For example:
\begin{align*}
	\mathbb E_t(X_{t+1}) &= \mathbb E(X_{t+1} \mid X_t) = \mathbb E(A X_t + Cw_{t+1} \mid X_t) \\
	&= A X_t + C \mathbb E(w_{t+1} \mid X_t) = A X_t + C \mathbb E(w_{t+1}) = A X_t
\end{align*}
\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\begin{witemize}
\item Rational expectations: agents' beliefs about stochastic processes are consistent with the true distribution of the process

\item Key equation: wealth dynamics with income fluctuations:
\begin{equation*}
	a_{t+1} = R_t a_t + y_t - c_t,
\end{equation*}
where $y_t$ is a stochastic process

\item Consumption Euler equation with uncertainty (e.g., stochastic income):
\begin{equation*}
	u'(C_t) = \beta R \mathbb E_t \Big[ u'(C_{t+1}) \Big]
\end{equation*}
\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{transitionframe}
	{\color{white} \huge \textbf{Part 2: Stochastic Dynamic Programming} \vspace{2mm}}
\end{transitionframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{1. Stochastic dynamic programming}
\begin{witemize}
\item Follow Ljungqvist-Sargent notation, Chapter 3.2 

\item Under uncertainty, household problem takes the form 
\begin{equation*}
	\max_{ \{c_t\} } \mathbb E_0 \sum_{t=0}^\infty \beta^t u(c_t)
\end{equation*}
subject to $k_{t+1} = g(k_t, c_t, \epsilon_{t+1})$ \hspace{1mm} (\textit{first-order stochastic difference equation})

\item Notice: $\{ c_t \}$ now denotes a stochastic process, no longer simple sequence!

\item $\{\epsilon_t\}_{t=0}^\infty$ is sequence of iid random variables (\textit{stochastic process})

\item Initial condition $x_0$ given
\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\begin{witemize}
\item Usually best to start with sequence problem, then derive recursive representation

\item To derive recursive representation, your first question must be:
\begin{itemize}
\item Recursive representation means we go from thinking about sequences (stochastic processes) to thinking about functions

\item But functions \textit{of what}? I.e., what is the domain of the functions we're interested in?

\item Answer: functions of the \textit{state variables}
\end{itemize}

\item What are state variables?
\begin{itemize}
\item In the (deterministic) neoclassical growth model: just $k$

\item Generally: state variables = set of information you need today to compute the continuation value for tomorrow

\item That's why they're called ``states''
\end{itemize}

\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\begin{witemize}
\item Dynamic programming: look for recursive representation with state variable $k$

\item Q: Why is $k$ the state variable here, not $(k, \epsilon)$? (Think about structure of $g(\cdot)$.)

\item The problem is to look for a \textit{policy function} $c(k)$ that solves 
\begin{equation*}
	V(k) = \max_{c} \bigg\{ u(c) + \beta \mathbb E \bigg[ V\Big( g(k, c, \epsilon) \Big) \, \Big| \, k \bigg] \bigg\}  ,
	\quad \text{ where }
	\mathbb E[V(\cdot) \mid k] = \int V(\cdot) dF(\epsilon)
\end{equation*}

\item $V(k)$ is (lifetime) value that agent obtains from solving this problem starting from $k$

\item FOC that characterizes the consumption policy function $c(k)$ is 
\begin{equation*}
	0 = u'(c(k)) + \beta \mathbb E \Big\{ \partial_k V \Big( g(k, c(k), \epsilon) \Big) \cdot \partial_c g(k, c(k), \epsilon) \, \Big| \, k \Big\} = 0
\end{equation*}
\end{witemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{2. History notation}
\begin{witemize}
\item A very popular approach to deal with uncertainty in macro is to use history notation (Ljungqvist-Sarget, e.g., chapters 8, 12)

\item Time is discrete and indexed by $t = 0, 1, \ldots$

\item At every $t$, there is a realization of a stochastic event $s_t \in \mathcal S$

\item We denote the \textbf{history} of such events up to $t$ by $s^t = \{s_0, s_1, \ldots, s_t\}$

\item The unconditional probability of history $s^t$ is given by $\pi_t(s^t \mid s_0)$

\item If Markov, $\pi_t( s^t \, | \, s_0) = \pi(s_t \mid s_{t-1}) \pi(s_{t-1} \mid s_{t-2}) \ldots \pi(s_0)$

\end{witemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\begin{witemize}
\item Crucial to understand notation:
\begin{itemize}
\item $\{ c_t \}_{t \geq 0}$ is the stochastic process
\item $c_t$ is the random variable
\item $c_t(s^t)$ is the realization of the random variable at date $t$ in history $s^t$
\end{itemize}

\item The \textbf{lifetime value} of representative household is then defined as
\begin{align*}
	V(s_0) = \sum_{t=0}^T \beta^t \sum_{s^t} \pi_t \Big( s^t \mid s_0 \Big) u \Big( c_t \Big( s^t \Big), \ell_t \Big(s^t \Big) \Big)
\end{align*}

\item Here we also allow household to choose labor supply $\ell_t$

\item \textit{Generalizations}: heterogeneous beliefs, general preferences (Epstein-Zin), recursive formulation, multiple commodities, intergenerational considerations

\end{witemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{3. Stochastic Growth Model}
\begin{witemize}
\item Discrete time: $t \in \{0, 1, \ldots, T\}$, where $T \leq \infty$

\item At $t$, event $s_t \in \mathcal{S}$ is realized; history $s^t = (s_0, \ldots, s_t)$ has probability $\pi_t(s^t)$

\item Representative household has preferences over consumption $c_t(s^t)$ and labor $\ell_t(s^t)$ 
\begin{equation*}
	\sum_{t=0}^\infty \beta^t \sum_{s^t} \pi_t\Big(s^t \Big) u \Big( c_t(s^t), \ell_t(s^t) \Big)
\end{equation*}

\item Inada conditions $\lim_{c\to 0} u_c(c, \ell) = \lim_{\ell \to0} u_\ell (c, \ell) = \infty$

\item At $t=0$, household endowed with $k_0$
\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\begin{witemize}
\item Technology, capital accumulation, and budget / resource constraint:
\begin{align*}
	c_t(s^t) + i_t(s^t) &= A_t(s^t) F(k_t(s^{t-1}), \ell_t(s^t)) \\
	k_{t+1}(s^t) &= (1-\delta) k_t(s^{t-1}) + i_t(s^t)
\end{align*}

\item $F(\cdot)$ is twice continuously differentiable and constant returns to scale

\item Source of uncertainty is stochastic process for TFP $A_t(s^t)$

\item Standard regularity conditions on $F(\cdot)$ (see LS)
\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Lagrangian approach to sequence problem}

\begin{align*}
	L = &\sum_{t=0}^\infty \sum_{s^t} \beta^t \pi_t(s^t) \bigg\{ u (c_t(s^t), l_t(s^t)) \\
	&+ \lambda_t(s^t) \bigg[ A_t(s^t) F(k_t(s^{t-1}), \ell_t(s^t)) - c_t(s^t) + (1-\delta) k_t(s^{t-1})  - k_{t+1}(s^t) \bigg] \bigg\}
\end{align*}

\begin{witemize}
\item FOCs for $c_t(s^t)$, $\ell_t(s^t)$ and $k_{t+1}(s^t)$ are given by
\begin{align*}
	u_c(s^t) &= \lambda_t(s^t) \\
	u_\ell(s^t) &= u_c(s^t) A_t(s^t) F_\ell(s^t) \\
	u_c(s^t) \pi_t(s^t) &= \beta \sum_{s^{t+1} \mid s^t} u_c(s^{t+1}) \pi_{t+1}(s^{t+1}) \bigg[ A_{t+1} (s^{t+1}) F_k (s^{t+1}) + (1-\delta) \bigg]
\end{align*}

\item Summation over $(s^{t+1} \mid s^t)$ is like conditional expectation 

	{\footnotesize summing over histories that branch out from $s^t$}

\end{witemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Dynamic programming approach}
\begin{witemize}
\item Assume time-homogeneous Markov process:
\begin{equation*}
	\mathbb E_t (A_{t+1}) = \mathbb E \Big[ A(s^{t+1}) \mid s^t \Big] = \mathbb E \Big[ A(s_{t+1}) \mid s_t \Big] = \sum_{s'} \pi(s' \mid s_t) A(s')
\end{equation*}

\item Drop $t$ subscripts: $s$ is current state, $s'$ denotes next period's draw

\item Denote by $X_t$ \textit{endogenous state} (assume for now there is such a representation)

\item Intuitively: $s$ is the exogenous state and $X$ is the endogenous state 
\end{witemize}

\vspace{5mm}
Bellman equation becomes: 
\begin{equation*}
	V(X, s) = \max_{c, \ell} \bigg\{ u(c, \ell) + \beta \sum_{s'} \pi(s' \mid s) V(X', s') \bigg\}
	\quad \text{ where }
	X' = g(X, c, \ell, s, s')
\end{equation*}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{transitionframe}
	{\color{white} \Huge \textbf{Part 3: Optimal Stopping} \vspace{2mm}}
\end{transitionframe}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Application: optimal stopping problem}

\textbf{Problem}: Every period $t$, an agent draws an offer $x$ from a uniform distributon over the unit interval $[0, 1]$. The agent can accept the offer, in which case her payoff is $x$, and the game ends, or the agent can reject the offer and draw again a period later. Draws are independent. Rejections are costly because the agent discounts the future at $\beta$. The game continues until the agent receives an offer she accepts.


\vspace{5mm}
Many applications (problems in life) look like this: 
\begin{itemize}
	\item buying a house
	\item searching for a partner
	\item closing a production plant
	\item exercising an option
	\item adopting a new technology
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}

What is recursive / dynamic programming representation of optimal stopping problem?

\pause
\vspace{5mm}
Agent's dynamic optimization problem given recursively by Bellman equation 
\begin{equation*}
	V(x) = \max \Big\{ x, \, \beta \mathbb{E} V(x') \Big\}
\end{equation*}
where the expectation (operator) $\mathbb{E}$ is taken over the next draw $x'$


\vspace{5mm} 
Our problem is to find the value function $V(x)$ that solves the Bellman equation. We'll also want to find the associated policy rule.

\vspace{6mm}
\textbf{Definition: }\textit{A policy is a function that maps every point in state space $[0, 1]$ to an action}

	{\footnotesize There are 2 actions: ACCEPT and REJECT}

\vspace{6mm}
\textbf{Definition: }\textit{An optimal policy achieves payoff $V(x)$ for all feasible $x \in [0, 1]$}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}

Let's try to understand the shape of $V(x)$ intuitively:
\vspace{2mm}
\begin{witemize}
\item For large values $\hat x$ where you ACCEPT, what's the value $V(\hat x)$?
\item For small values $\tilde x$ where you REJECT and instead choose the continuation value, $\beta \mathbb E V(x') > \tilde x$, does the continuation value depend on $\tilde x$? Why not?
\end{witemize}

\pause
\vspace{5mm}
Shape of $V(x)$ must therefore be:
\begin{equation*}
	V(x) = 
	\begin{cases} 
		x & \text{ if } \;\; x \geq x^* \\
		x^* & \text{ if } \;\; x < x^* 
	\end{cases}
\end{equation*}
\begin{witemize}
\item Solution to the problem: there is be threshold $x^* \in [0, 1]$ s.t. agent accepts for $x \geq x^*$

\item Also called \textbf{free boundary problem} (have to find endogenous boundary $x^*$)

\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}

\vspace{5mm}
\textbf{Lemma}: \textit{In the optimal stopping problem, a policy is a best response to a continuation value function $\widehat v(x)$ if and only if the policy is a threshold rule
with cutoff} 
\begin{equation*}
	x^* \equiv \beta \mathbb E[ \widehat v(x')]  
\end{equation*}

\vspace{5mm}
\textbf{Proof:} Show by contradiction that optimization must imply 
\begin{equation*}
\begin{array}{ccc}
	\text{ACCEPT} & \text{if} & x > \beta \mathbb E[ \widehat{v}(x')] \equiv x^* \\
	\text{REJECT} & \text{if} & x < \beta \mathbb E[ \widehat{v}(x')] \equiv x^*
\end{array}
\end{equation*}
If $x = \beta \mathbb E[\widehat v(x')]$, then ACCEPT and REJECT generate the same payoff. $\blacksquare $

\vspace{4mm}
\begin{itemize}
\item Why no jump in $V(x)$ at $x^*$? (lim from RHS must be $x^*$, from LHS by contradiction)

\item Continuation value must be $V(x')$ because problem tmr is repeat of today
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}

\vspace{6mm}
\begin{witemize}
\item We just concluded: at $x = x^*$, indifferent between ACCEPT and REJECT

\item This is enough information to solve the problem!
\end{witemize}
\begin{align*}
	V(x^*) &= x^* \\
	&= \beta \mathbb{E} V(x') \\
	&= \beta \int_0^{x^*} x^* f(x) dx + \beta \int_{x^*}^1 x f(x) dx \\
	&= \beta x^*  [x]_0^{x^*} + \beta \frac{1}{2} [x^2]_{x^*}^1 \\
	&= \beta (x^*)^2 + \beta \frac{1}{2} [1 - (x^*)^2]
\end{align*}

\vspace{5mm}
\textbf{Solution:} 

\vspace{-10.4mm}
\begin{equation*}
	x^* = \frac{1}{\beta} (1 - \sqrt{1 - \beta^2})
\end{equation*}

	{\footnotesize Always sanity-check comparative statics: What happens as $\beta \to 0$ and $\beta \to 1$?}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}

Why is this threshold rule a \textit{solution to the Bellman Equation}?

If you REJECT, your continuation payoff is 
\begin{equation*}
	x^* = \beta \mathbb E V(x') = \beta \int_0^{x^*} x^* f(x) \, dx + \beta \int_{x^*}^1 x \, f(x) \, dx. 
\end{equation*}
So it's optimal to REJECT if $x \leq x^*$ and it's optimal to ACCEPT if $x \geq x^*$. Hence, for all values of $x$
\begin{equation*}
	V(x) = \max \{x, x^*\} = \max \{x, \beta \mathbb E[ V(x') ]\} 
\end{equation*}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Application: job market paper}

\textbf{Problem}: Imagine you were a graduate student in economics. And you have to find your job market paper. Time is discrete, and you have to go on the job market in period $T$. 

\pause
\vspace{3mm}
Your current project has quality $x$. Each period, you allocate your time to (i) work on your current project or (ii) search for a new project. Your project quality evolves as:
\pause
\begin{equation*}
	x' = 
	\begin{cases}
		f(x, \epsilon) & \text{ where } \epsilon \sim \log \mathcal N(\mu, \sigma)  \\
		\epsilon^{new} &\text{ where } \epsilon^{new} \sim \log \mathcal N(\mu^{new}, \sigma^{new})
	\end{cases}
\end{equation*}

Your goal is to go on the market with the highest-quality project. 

\pause
\vspace{5mm}
What is the Bellman equation for this problem?

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}

\begin{witemize}
\item This is an example of \textbf{non-stationary dynamic programming}

\item Let's start at the end: What is your payoff when you go on the market in period $T$?
\begin{equation*}
	V_T(x) = x
\end{equation*}

\item What is your value in period $T-1$?
\begin{align*}
	V_{T-1}(x) 
	&= \max \Big\{\mathbb E V_T\Big( f(x, \epsilon)\Big) , \mathbb E V_T(\epsilon^{new}) \Big\} \\
	&= \max \Big\{\mathbb E f(x, \epsilon), \mu^{new} \Big\}
\end{align*}

\item Iterate backwards until 
\begin{equation*}
	V_0(x) = \max \Big\{\mathbb E V_1\Big( f(x, \epsilon)\Big) , \mathbb E V_1(\epsilon^{new}) \Big\} 
\end{equation*}
\end{witemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}

\begin{witemize}
\item This is an easy problem to solve on the computer! (More examples to come in class)

	{\footnotesize You can probably solve this by hand but I haven't tried. Give it a shot for simple $f(\cdot)$!}

\item Interpret the economics:
\begin{itemize}
\item How to think about the length of the time step? (How large is $T$?)
\item How to think about $f(x, \epsilon)$?
\item Is $\sigma^{new}$ larger or smaller than $\sigma$?
\item Should $\sigma$ maybe also be a state variable (and change over time)?
\end{itemize}

\item Key takeaway from this sort of problem: $\sigma^{new} >> 0$ is your best friend and much more important than $\mu^{new}$!!! (Easy to check on computer!)

\item While $T$ is still large, look for the riskiest projects you can think of!!! 
\end{witemize}
\end{frame}


\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}

\begin{witemize}
\item Solve by hand?

\item Once you choose WORK, you will never choose SEARCH again

\item Suppose you stick with a project of quality $x_t$ at date $t$, then 

\item Therefore, each period $t$ there exists $x_t^*$ such that optimal policy is
\begin{equation*}
	\begin{cases}
		\text{WORK} & \text{ if } x > x_t^* \\
		\text{SEARCH} & \text{ if } x \leq x_t^* 
	\end{cases}
	\quad \implies \quad
	V_t(x) =
	\begin{cases}
		NPV_t(x) & \text{ if } x > x_t^* \\
		x_t^* & \text{ if } x \leq x_t^*
	\end{cases}
\end{equation*}
\end{witemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Optimal stopping: Example 1}

\textbf{Problem:}
XXX

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Optimal stopping: Example 2}

\textbf{Problem:}
A burglar contemplates a series of burglaries. He may accumulate his larcenous earnings as long as he is not caught, but if he is caught during a burglary, he loses everything including his initial fortune, if any, and he is forced to retire. He wants to retire before he is caught. Assume that returns for each burglary are iid. and independent of the event that he is caught, which is, on each trial, equally probable. He wants to retire with a maximum expected fortune.


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Optimal stopping: Example 3}

\textbf{Problem:}
XXX

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Optimal stopping: Example 4}

\textbf{Problem:}
XXX

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%  SLIDE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Optimal stopping: Example 5}

\textbf{Problem:}
XXX

\end{frame}




\end{document}



















